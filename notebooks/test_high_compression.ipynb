{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6a8fc273a14f35bec86537490d06cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map = \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(\n",
    "    model,\n",
    "    target_module = \"self_attn\",\n",
    "    target_module_suffixes = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "):\n",
    "\n",
    "    names = []\n",
    "    for name, _ in model.named_modules():\n",
    "        if target_module in name:\n",
    "            for suffix in target_module_suffixes:\n",
    "                if suffix in name:\n",
    "                    names.append(name)\n",
    "\n",
    "    return names\n",
    "\n",
    "linear_module_names = find_all_linear_names(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _low_rank_decomposition(\n",
    "    weight,\n",
    "    reduced_rank=128,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    full_matrices = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Decompose a 2D matrix into low-rank matrices A and B using SVD.a\n",
    "\n",
    "    :param weight: The matrix to decompose, of shape (H, W)\n",
    "    :param reduced_rank: The final rank of the decomposition\n",
    "    :return: A tuple of tensors (A, B)\n",
    "    \"\"\"\n",
    "    if weight.dim() != 2:\n",
    "        raise ValueError(f\"Only support 2D matrix, but your input has {weight.dim()} dimensions.\")\n",
    "\n",
    "    # SVD Decomposition\n",
    "    U, S, Vh = torch.linalg.svd(weight, full_matrices=full_matrices)\n",
    "\n",
    "    # Truncated matrices\n",
    "    A = Vh[:reduced_rank, :]\n",
    "    B = U[:, :reduced_rank] @ torch.diag(S[:reduced_rank])\n",
    "\n",
    "    if torch_dtype:\n",
    "        A = A.to(torch_dtype)\n",
    "        B = B.to(torch_dtype)\n",
    "\n",
    "    return A, B\n",
    "\n",
    "def decompose_delta_weight(new_weight, base_weight, alpha, reduced_rank, device=None):\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    new_weight = new_weight.to(device)\n",
    "    base_weight = base_weight.to(device)\n",
    "\n",
    "    \"\"\"\n",
    "    Decompose the delta weight into low-rank matrices A and B, considering the alpha scaling factor.\n",
    "\n",
    "    :param new_weight: The updated weight matrix after applying LoRA.\n",
    "    :param base_weight: The original weight matrix before LoRA.\n",
    "    :param alpha: The alpha scaling factor used in LoRA.\n",
    "    :param reduced_rank: The rank for the low-rank decomposition.\n",
    "    :return: A tuple of tensors (A, B)\n",
    "    \"\"\"\n",
    "    delta_weight = new_weight - base_weight\n",
    "\n",
    "    del new_weight\n",
    "    del base_weight\n",
    "\n",
    "    # Check if alpha is applied uniformly\n",
    "    # Adjust the implementation if alpha is applied differently\n",
    "    adjusted_delta_weight = delta_weight / alpha\n",
    "\n",
    "    del delta_weight\n",
    "\n",
    "    A, B = _low_rank_decomposition(adjusted_delta_weight, reduced_rank=reduced_rank)\n",
    "\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft.tuners.tuners_utils import replicate_layers\n",
    "import copy\n",
    "\n",
    "base_model = copy.deepcopy(target_model.to(\"cpu\"))\n",
    "\n",
    "\"\"\"\n",
    "Replace the following layers:\n",
    "22\tlayer_29\t0.159546\n",
    "11\tlayer_19\t0.137939\n",
    "8\tlayer_16\t0.084167\n",
    "7\tlayer_15\t0.083313\n",
    "27\tlayer_5\t0.082794\n",
    "21\tlayer_28\t0.081604\n",
    "26\tlayer_4\t0.080139\n",
    "14\tlayer_21\t0.079468\n",
    "\"\"\"\n",
    "\n",
    "layer_map = [\n",
    "    [0, 4],\n",
    "    [3,4],\n",
    "    [6,7],\n",
    "    [6, 15],\n",
    "    [14, 15],\n",
    "    [17, 18],\n",
    "    [17, 19],\n",
    "    [18, 19],\n",
    "    [20, 21],\n",
    "    [20, 21],\n",
    "    [22, 28],\n",
    "    [27, 28],\n",
    "    [30, 31],\n",
    "    [30, 31],\n",
    "    [31, 32]\n",
    "]\n",
    "\n",
    "replicate_layers(model = base_model,layer_map = layer_map)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb370de2e52441ea589f46a3b55369e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "loras = OrderedDict()\n",
    "\n",
    "# lower rank captures less of the original model, a rank of 32 is probably reasonable for small delta (task specific finetunes and such)\n",
    "alpha = 16\n",
    "rank = 128\n",
    "\n",
    "for module in tqdm(linear_module_names):\n",
    "  \n",
    "  target_tensor = target_model.state_dict()[module+\".weight\"]\n",
    "  base_tensor = base_model.state_dict()[module+\".weight\"]\n",
    "\n",
    "  lora_A, lora_B = decompose_delta_weight(target_tensor, base_tensor, alpha, rank)\n",
    "  loras[f\"base_model.model.{module}.lora_A.weight\"] = lora_A.to('cpu')\n",
    "  loras[f\"base_model.model.{module}.lora_B.weight\"] = lora_B.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import os\n",
    "\n",
    "LORA_OUT_DIR = \"./lora\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "        lora_alpha=alpha, # Setting the alpha to the to decomposition rank value (instead of alpha value used) seems to give better performance. Further testing would be needed to understand what is the optimal alpha value to use\n",
    "        lora_dropout=0.1, # Dropout = 0.1 makes a huge diff compared to 0.0!\n",
    "        r=rank,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "lora_config.save_pretrained(LORA_OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for key in loras.keys():\n",
    "    loras[key] = loras[key].to('cpu').contiguous()\n",
    "\n",
    "torch.save(loras, os.path.join(LORA_OUT_DIR, 'adapter_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Save to disk\n",
    "peft_model.save_pretrained(LORA_OUT_DIR)\n",
    "\n",
    "del peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "LORA_OUT_DIR = \"./lora\"\n",
    "target_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(os.path.abspath(LORA_OUT_DIR))\n",
    "model = PeftModel.from_pretrained(base_model, os.path.abspath(LORA_OUT_DIR), device_map = \"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] You are a helpful assistant. Please answer the questions below in a sound and truthful way:\n",
      "What are great things to do in Catania, Sicily? [/INST] Catania is a beautiful city with many great things worth doing. Here are some of the top options:\n",
      "\n",
      "1. Visit the Basilica of Santa Maria Lucia, which is a UNESCO World Heritage site and the largest church in the Mediterranean.\n",
      "2. Take a stroll on the beach of the city, where you can enjoy the beautiful views of the Mediterranean and relax.\n",
      "3. Visit one of the many historic sites, such as the Church of St. Maria da Assisi, the Church on the Hill, or the Church in the Woods.\n",
      "4. Take in the stunning views of Old Aquatitaino Cemetery, which dates back to the 13th century.\n",
      "5. Take part in a wine-tasting tour of the area, where visitors can sample the famous Sicolian wine.\n",
      "6. Take advantage of the rich cultural heritage of the region, and visit the local museums and art galleries.\n",
      "7. Take to the beautiful beaches of the island, where the visitors can enjoy a stunning view of the sea and the beautiful scenery.\n",
      "8. Take the time to explore the beautiful city of Catania and its surroundings, which are rich in history and culture.\n",
      "9. Take time to visit the beautiful island of Sicily, which has a rich history and cultural heritage.\n",
      "10. Take your time to enjoy the stunning beauty of the islands and the stunning scenerry of the beautiful islands.</s>\n"
     ]
    }
   ],
   "source": [
    "# Test input\n",
    "\n",
    "placeholder = \"You are a helpful assistant. Please answer the questions below in a sound and truthful way:\\n{question}\"\n",
    "instruction = placeholder.format(\n",
    "    question = \"What are great things to do in Catania, Sicily?\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": instruction},\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Tokenize and format the chat for the model\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "model = model.bfloat16().cuda()\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=2048, do_sample = True, temperature = 0.1, top_p = 0.95, no_repeat_ngram_size = 3)  # Adjust max_new_tokens if needed\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.496836096"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in merged.parameters())\n",
    "total_params / 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(merged.state_dict(), \"models/merged/state_dict.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmin-WyFAvJN6-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
