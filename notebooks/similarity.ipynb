{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef3c34ad1154f75a93f45d9c7128eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llmin.similarity import sim_matrix\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "target_model = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    target_model,\n",
    "    device_map = \"cuda\",\n",
    "    torch_dtype = torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_similarities(\n",
    "    linear_module_name: str = \"model.layers.{idx}.self_attn.q_proj.weight\",\n",
    "    num_layers = 32\n",
    "):\n",
    "\n",
    "    similarities = []\n",
    "    for i in tqdm(range(0, num_layers)):\n",
    "        for j in range(0, num_layers):\n",
    "            if i != j:\n",
    "                sim = sim_matrix(\n",
    "                    a = model.state_dict()[linear_module_name.format(idx = i)],\n",
    "                    b = model.state_dict()[linear_module_name.format(idx = j)]\n",
    "                ).to(\"cpu\")\n",
    "                idx = (sim==torch.max(sim)).nonzero()[0].to(\"cpu\")\n",
    "                similarities.append({\n",
    "                    \"sim\": sim[idx[0].item(), idx[1].item()],\n",
    "                    \"layer_1\": linear_module_name.format(idx = i),\n",
    "                    \"layer_2\": linear_module_name.format(idx = j)\n",
    "                })\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.0.self_attn.q_proj',\n",
       " 'model.layers.0.self_attn.k_proj',\n",
       " 'model.layers.0.self_attn.v_proj',\n",
       " 'model.layers.0.self_attn.o_proj',\n",
       " 'model.layers.0.mlp.gate_proj',\n",
       " 'model.layers.0.mlp.up_proj',\n",
       " 'model.layers.0.mlp.down_proj',\n",
       " 'model.layers.1.self_attn.q_proj',\n",
       " 'model.layers.1.self_attn.k_proj',\n",
       " 'model.layers.1.self_attn.v_proj',\n",
       " 'model.layers.1.self_attn.o_proj',\n",
       " 'model.layers.1.mlp.gate_proj',\n",
       " 'model.layers.1.mlp.up_proj',\n",
       " 'model.layers.1.mlp.down_proj',\n",
       " 'model.layers.2.self_attn.q_proj',\n",
       " 'model.layers.2.self_attn.k_proj',\n",
       " 'model.layers.2.self_attn.v_proj',\n",
       " 'model.layers.2.self_attn.o_proj',\n",
       " 'model.layers.2.mlp.gate_proj',\n",
       " 'model.layers.2.mlp.up_proj',\n",
       " 'model.layers.2.mlp.down_proj',\n",
       " 'model.layers.3.self_attn.q_proj',\n",
       " 'model.layers.3.self_attn.k_proj',\n",
       " 'model.layers.3.self_attn.v_proj',\n",
       " 'model.layers.3.self_attn.o_proj',\n",
       " 'model.layers.3.mlp.gate_proj',\n",
       " 'model.layers.3.mlp.up_proj',\n",
       " 'model.layers.3.mlp.down_proj',\n",
       " 'model.layers.4.self_attn.q_proj',\n",
       " 'model.layers.4.self_attn.k_proj',\n",
       " 'model.layers.4.self_attn.v_proj',\n",
       " 'model.layers.4.self_attn.o_proj',\n",
       " 'model.layers.4.mlp.gate_proj',\n",
       " 'model.layers.4.mlp.up_proj',\n",
       " 'model.layers.4.mlp.down_proj',\n",
       " 'model.layers.5.self_attn.q_proj',\n",
       " 'model.layers.5.self_attn.k_proj',\n",
       " 'model.layers.5.self_attn.v_proj',\n",
       " 'model.layers.5.self_attn.o_proj',\n",
       " 'model.layers.5.mlp.gate_proj',\n",
       " 'model.layers.5.mlp.up_proj',\n",
       " 'model.layers.5.mlp.down_proj',\n",
       " 'model.layers.6.self_attn.q_proj',\n",
       " 'model.layers.6.self_attn.k_proj',\n",
       " 'model.layers.6.self_attn.v_proj',\n",
       " 'model.layers.6.self_attn.o_proj',\n",
       " 'model.layers.6.mlp.gate_proj',\n",
       " 'model.layers.6.mlp.up_proj',\n",
       " 'model.layers.6.mlp.down_proj',\n",
       " 'model.layers.7.self_attn.q_proj',\n",
       " 'model.layers.7.self_attn.k_proj',\n",
       " 'model.layers.7.self_attn.v_proj',\n",
       " 'model.layers.7.self_attn.o_proj',\n",
       " 'model.layers.7.mlp.gate_proj',\n",
       " 'model.layers.7.mlp.up_proj',\n",
       " 'model.layers.7.mlp.down_proj',\n",
       " 'model.layers.8.self_attn.q_proj',\n",
       " 'model.layers.8.self_attn.k_proj',\n",
       " 'model.layers.8.self_attn.v_proj',\n",
       " 'model.layers.8.self_attn.o_proj',\n",
       " 'model.layers.8.mlp.gate_proj',\n",
       " 'model.layers.8.mlp.up_proj',\n",
       " 'model.layers.8.mlp.down_proj',\n",
       " 'model.layers.9.self_attn.q_proj',\n",
       " 'model.layers.9.self_attn.k_proj',\n",
       " 'model.layers.9.self_attn.v_proj',\n",
       " 'model.layers.9.self_attn.o_proj',\n",
       " 'model.layers.9.mlp.gate_proj',\n",
       " 'model.layers.9.mlp.up_proj',\n",
       " 'model.layers.9.mlp.down_proj',\n",
       " 'model.layers.10.self_attn.q_proj',\n",
       " 'model.layers.10.self_attn.k_proj',\n",
       " 'model.layers.10.self_attn.v_proj',\n",
       " 'model.layers.10.self_attn.o_proj',\n",
       " 'model.layers.10.mlp.gate_proj',\n",
       " 'model.layers.10.mlp.up_proj',\n",
       " 'model.layers.10.mlp.down_proj',\n",
       " 'model.layers.11.self_attn.q_proj',\n",
       " 'model.layers.11.self_attn.k_proj',\n",
       " 'model.layers.11.self_attn.v_proj',\n",
       " 'model.layers.11.self_attn.o_proj',\n",
       " 'model.layers.11.mlp.gate_proj',\n",
       " 'model.layers.11.mlp.up_proj',\n",
       " 'model.layers.11.mlp.down_proj',\n",
       " 'model.layers.12.self_attn.q_proj',\n",
       " 'model.layers.12.self_attn.k_proj',\n",
       " 'model.layers.12.self_attn.v_proj',\n",
       " 'model.layers.12.self_attn.o_proj',\n",
       " 'model.layers.12.mlp.gate_proj',\n",
       " 'model.layers.12.mlp.up_proj',\n",
       " 'model.layers.12.mlp.down_proj',\n",
       " 'model.layers.13.self_attn.q_proj',\n",
       " 'model.layers.13.self_attn.k_proj',\n",
       " 'model.layers.13.self_attn.v_proj',\n",
       " 'model.layers.13.self_attn.o_proj',\n",
       " 'model.layers.13.mlp.gate_proj',\n",
       " 'model.layers.13.mlp.up_proj',\n",
       " 'model.layers.13.mlp.down_proj',\n",
       " 'model.layers.14.self_attn.q_proj',\n",
       " 'model.layers.14.self_attn.k_proj',\n",
       " 'model.layers.14.self_attn.v_proj',\n",
       " 'model.layers.14.self_attn.o_proj',\n",
       " 'model.layers.14.mlp.gate_proj',\n",
       " 'model.layers.14.mlp.up_proj',\n",
       " 'model.layers.14.mlp.down_proj',\n",
       " 'model.layers.15.self_attn.q_proj',\n",
       " 'model.layers.15.self_attn.k_proj',\n",
       " 'model.layers.15.self_attn.v_proj',\n",
       " 'model.layers.15.self_attn.o_proj',\n",
       " 'model.layers.15.mlp.gate_proj',\n",
       " 'model.layers.15.mlp.up_proj',\n",
       " 'model.layers.15.mlp.down_proj',\n",
       " 'model.layers.16.self_attn.q_proj',\n",
       " 'model.layers.16.self_attn.k_proj',\n",
       " 'model.layers.16.self_attn.v_proj',\n",
       " 'model.layers.16.self_attn.o_proj',\n",
       " 'model.layers.16.mlp.gate_proj',\n",
       " 'model.layers.16.mlp.up_proj',\n",
       " 'model.layers.16.mlp.down_proj',\n",
       " 'model.layers.17.self_attn.q_proj',\n",
       " 'model.layers.17.self_attn.k_proj',\n",
       " 'model.layers.17.self_attn.v_proj',\n",
       " 'model.layers.17.self_attn.o_proj',\n",
       " 'model.layers.17.mlp.gate_proj',\n",
       " 'model.layers.17.mlp.up_proj',\n",
       " 'model.layers.17.mlp.down_proj',\n",
       " 'model.layers.18.self_attn.q_proj',\n",
       " 'model.layers.18.self_attn.k_proj',\n",
       " 'model.layers.18.self_attn.v_proj',\n",
       " 'model.layers.18.self_attn.o_proj',\n",
       " 'model.layers.18.mlp.gate_proj',\n",
       " 'model.layers.18.mlp.up_proj',\n",
       " 'model.layers.18.mlp.down_proj',\n",
       " 'model.layers.19.self_attn.q_proj',\n",
       " 'model.layers.19.self_attn.k_proj',\n",
       " 'model.layers.19.self_attn.v_proj',\n",
       " 'model.layers.19.self_attn.o_proj',\n",
       " 'model.layers.19.mlp.gate_proj',\n",
       " 'model.layers.19.mlp.up_proj',\n",
       " 'model.layers.19.mlp.down_proj',\n",
       " 'model.layers.20.self_attn.q_proj',\n",
       " 'model.layers.20.self_attn.k_proj',\n",
       " 'model.layers.20.self_attn.v_proj',\n",
       " 'model.layers.20.self_attn.o_proj',\n",
       " 'model.layers.20.mlp.gate_proj',\n",
       " 'model.layers.20.mlp.up_proj',\n",
       " 'model.layers.20.mlp.down_proj',\n",
       " 'model.layers.21.self_attn.q_proj',\n",
       " 'model.layers.21.self_attn.k_proj',\n",
       " 'model.layers.21.self_attn.v_proj',\n",
       " 'model.layers.21.self_attn.o_proj',\n",
       " 'model.layers.21.mlp.gate_proj',\n",
       " 'model.layers.21.mlp.up_proj',\n",
       " 'model.layers.21.mlp.down_proj',\n",
       " 'model.layers.22.self_attn.q_proj',\n",
       " 'model.layers.22.self_attn.k_proj',\n",
       " 'model.layers.22.self_attn.v_proj',\n",
       " 'model.layers.22.self_attn.o_proj',\n",
       " 'model.layers.22.mlp.gate_proj',\n",
       " 'model.layers.22.mlp.up_proj',\n",
       " 'model.layers.22.mlp.down_proj',\n",
       " 'model.layers.23.self_attn.q_proj',\n",
       " 'model.layers.23.self_attn.k_proj',\n",
       " 'model.layers.23.self_attn.v_proj',\n",
       " 'model.layers.23.self_attn.o_proj',\n",
       " 'model.layers.23.mlp.gate_proj',\n",
       " 'model.layers.23.mlp.up_proj',\n",
       " 'model.layers.23.mlp.down_proj',\n",
       " 'model.layers.24.self_attn.q_proj',\n",
       " 'model.layers.24.self_attn.k_proj',\n",
       " 'model.layers.24.self_attn.v_proj',\n",
       " 'model.layers.24.self_attn.o_proj',\n",
       " 'model.layers.24.mlp.gate_proj',\n",
       " 'model.layers.24.mlp.up_proj',\n",
       " 'model.layers.24.mlp.down_proj',\n",
       " 'model.layers.25.self_attn.q_proj',\n",
       " 'model.layers.25.self_attn.k_proj',\n",
       " 'model.layers.25.self_attn.v_proj',\n",
       " 'model.layers.25.self_attn.o_proj',\n",
       " 'model.layers.25.mlp.gate_proj',\n",
       " 'model.layers.25.mlp.up_proj',\n",
       " 'model.layers.25.mlp.down_proj',\n",
       " 'model.layers.26.self_attn.q_proj',\n",
       " 'model.layers.26.self_attn.k_proj',\n",
       " 'model.layers.26.self_attn.v_proj',\n",
       " 'model.layers.26.self_attn.o_proj',\n",
       " 'model.layers.26.mlp.gate_proj',\n",
       " 'model.layers.26.mlp.up_proj',\n",
       " 'model.layers.26.mlp.down_proj',\n",
       " 'model.layers.27.self_attn.q_proj',\n",
       " 'model.layers.27.self_attn.k_proj',\n",
       " 'model.layers.27.self_attn.v_proj',\n",
       " 'model.layers.27.self_attn.o_proj',\n",
       " 'model.layers.27.mlp.gate_proj',\n",
       " 'model.layers.27.mlp.up_proj',\n",
       " 'model.layers.27.mlp.down_proj',\n",
       " 'model.layers.28.self_attn.q_proj',\n",
       " 'model.layers.28.self_attn.k_proj',\n",
       " 'model.layers.28.self_attn.v_proj',\n",
       " 'model.layers.28.self_attn.o_proj',\n",
       " 'model.layers.28.mlp.gate_proj',\n",
       " 'model.layers.28.mlp.up_proj',\n",
       " 'model.layers.28.mlp.down_proj',\n",
       " 'model.layers.29.self_attn.q_proj',\n",
       " 'model.layers.29.self_attn.k_proj',\n",
       " 'model.layers.29.self_attn.v_proj',\n",
       " 'model.layers.29.self_attn.o_proj',\n",
       " 'model.layers.29.mlp.gate_proj',\n",
       " 'model.layers.29.mlp.up_proj',\n",
       " 'model.layers.29.mlp.down_proj',\n",
       " 'model.layers.30.self_attn.q_proj',\n",
       " 'model.layers.30.self_attn.k_proj',\n",
       " 'model.layers.30.self_attn.v_proj',\n",
       " 'model.layers.30.self_attn.o_proj',\n",
       " 'model.layers.30.mlp.gate_proj',\n",
       " 'model.layers.30.mlp.up_proj',\n",
       " 'model.layers.30.mlp.down_proj',\n",
       " 'model.layers.31.self_attn.q_proj',\n",
       " 'model.layers.31.self_attn.k_proj',\n",
       " 'model.layers.31.self_attn.v_proj',\n",
       " 'model.layers.31.self_attn.o_proj',\n",
       " 'model.layers.31.mlp.gate_proj',\n",
       " 'model.layers.31.mlp.up_proj',\n",
       " 'model.layers.31.mlp.down_proj',\n",
       " 'lm_head']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_similarities = []\n",
    "\n",
    "modules = [\n",
    "    \"self_attn.q_proj\",\n",
    "    \"self_attn.k_proj\",\n",
    "    \"self_attn.v_proj\",\n",
    "    \"self_attn.o_proj\",\n",
    "    \"mlp.gate_proj\",\n",
    "    \"mlp.up_proj\",\n",
    "    \"mlp.down_proj\"\n",
    "]\n",
    "\n",
    "for module in modules:\n",
    "    module_name = \"model.layers.{idx}.\" + module + \".weight\"\n",
    "    similarities = calculate_similarities(linear_module_name = module_name)\n",
    "    sorted_similarities = sorted(similarities, key = lambda item: item[\"sim\"], reverse = True)\n",
    "    layer_similarities.append(sorted_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_similarities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmin-WyFAvJN6-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
