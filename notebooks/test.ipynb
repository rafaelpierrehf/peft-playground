{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map = \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(\n",
    "    model,\n",
    "    target_module = \"self_attn\",\n",
    "    target_module_suffixes = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "):\n",
    "\n",
    "    names = []\n",
    "    for name, _ in model.named_modules():\n",
    "        if target_module in name:\n",
    "            for suffix in target_module_suffixes:\n",
    "                if suffix in name:\n",
    "                    names.append(name)\n",
    "\n",
    "    return names\n",
    "\n",
    "linear_module_names = find_all_linear_names(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _low_rank_decomposition(\n",
    "    weight,\n",
    "    reduced_rank=64,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    full_matrices = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Decompose a 2D matrix into low-rank matrices A and B using SVD.a\n",
    "\n",
    "    :param weight: The matrix to decompose, of shape (H, W)\n",
    "    :param reduced_rank: The final rank of the decomposition\n",
    "    :return: A tuple of tensors (A, B)\n",
    "    \"\"\"\n",
    "    if weight.dim() != 2:\n",
    "        raise ValueError(f\"Only support 2D matrix, but your input has {weight.dim()} dimensions.\")\n",
    "\n",
    "    # SVD Decomposition\n",
    "    U, S, Vh = torch.linalg.svd(weight, full_matrices=full_matrices)\n",
    "\n",
    "    # Truncated matrices\n",
    "    A = Vh[:reduced_rank, :]\n",
    "    B = U[:, :reduced_rank] @ torch.diag(S[:reduced_rank])\n",
    "\n",
    "    if torch_dtype:\n",
    "        A = A.to(torch_dtype)\n",
    "        B = B.to(torch_dtype)\n",
    "\n",
    "    return A, B\n",
    "\n",
    "def decompose_delta_weight(new_weight, base_weight, alpha, reduced_rank, device=None):\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    new_weight = new_weight.to(device)\n",
    "    base_weight = base_weight.to(device)\n",
    "\n",
    "    \"\"\"\n",
    "    Decompose the delta weight into low-rank matrices A and B, considering the alpha scaling factor.\n",
    "\n",
    "    :param new_weight: The updated weight matrix after applying LoRA.\n",
    "    :param base_weight: The original weight matrix before LoRA.\n",
    "    :param alpha: The alpha scaling factor used in LoRA.\n",
    "    :param reduced_rank: The rank for the low-rank decomposition.\n",
    "    :return: A tuple of tensors (A, B)\n",
    "    \"\"\"\n",
    "    delta_weight = new_weight - base_weight\n",
    "\n",
    "    del new_weight\n",
    "    del base_weight\n",
    "\n",
    "    # Check if alpha is applied uniformly\n",
    "    # Adjust the implementation if alpha is applied differently\n",
    "    adjusted_delta_weight = delta_weight / alpha\n",
    "\n",
    "    del delta_weight\n",
    "\n",
    "    A, B = _low_rank_decomposition(adjusted_delta_weight, reduced_rank=reduced_rank)\n",
    "\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.tuners_utils import replicate_layers\n",
    "import copy\n",
    "\n",
    "base_model = copy.deepcopy(target_model.to(\"cpu\"))\n",
    "\n",
    "layer_map = [\n",
    "    [0, 8],\n",
    "    [13, 14],\n",
    "    [10, 12],\n",
    "    [13, 16],\n",
    "    [13, 14],\n",
    "    [14, 28],\n",
    "    [13, 14],\n",
    "    [13, 14],\n",
    "    [30, 32]\n",
    "]\n",
    "\n",
    "\n",
    "replicate_layers(model = base_model,layer_map = layer_map)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "loras = OrderedDict()\n",
    "\n",
    "# lower rank captures less of the original model, a rank of 32 is probably reasonable for small delta (task specific finetunes and such)\n",
    "alpha = 16\n",
    "rank = 64\n",
    "\n",
    "for module in tqdm(linear_module_names):\n",
    "  \n",
    "  target_tensor = target_model.state_dict()[module+\".weight\"]\n",
    "  base_tensor = base_model.state_dict()[module+\".weight\"]\n",
    "\n",
    "  lora_A, lora_B = decompose_delta_weight(target_tensor, base_tensor, alpha, rank)\n",
    "  loras[f\"base_model.model.{module}.lora_A.weight\"] = lora_A.to('cpu')\n",
    "  loras[f\"base_model.model.{module}.lora_B.weight\"] = lora_B.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import os\n",
    "\n",
    "LORA_OUT_DIR = \"./lora\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "        lora_alpha=alpha, # Setting the alpha to the to decomposition rank value (instead of alpha value used) seems to give better performance. Further testing would be needed to understand what is the optimal alpha value to use\n",
    "        lora_dropout=0, #TODO: experiment with dropout\n",
    "        r=rank,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "lora_config.save_pretrained(LORA_OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for key in loras.keys():\n",
    "    loras[key] = loras[key].to('cpu').contiguous()\n",
    "\n",
    "torch.save(loras, os.path.join(LORA_OUT_DIR, 'adapter_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Save to disk\n",
    "peft_model.save_pretrained(LORA_OUT_DIR)\n",
    "\n",
    "del peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "LORA_OUT_DIR = \"./lora\"\n",
    "target_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(os.path.abspath(LORA_OUT_DIR))\n",
    "model = PeftModel.from_pretrained(base_model, os.path.abspath(LORA_OUT_DIR), device_map = \"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test input\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hey what's up?\"},\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Tokenize and format the chat for the model\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "model = model.bfloat16().cuda()\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=256, do_sample = True, top_p = 0.95, temperature = 0.6)  # Adjust max_new_tokens if needed\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
