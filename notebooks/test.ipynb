{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "import os\n",
    "from huggingface_hub import list_repo_files, snapshot_download\n",
    "\n",
    "target_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "def download_transformers_model(repo_id, cache_dir=None):\n",
    "    # Check for .safetensors files in the repository\n",
    "    repo_files = list_repo_files(repo_id)\n",
    "    has_safetensors = any(file.endswith('.safetensors') for file in repo_files)\n",
    "\n",
    "    # Define ignore_patterns based on the presence of .safetensors files\n",
    "    ignore_patterns = [\"*.bin\"] if has_safetensors else None\n",
    "\n",
    "    # Download the repository, ignoring PyTorch .bin files if .safetensors files are present\n",
    "    local_path = snapshot_download(repo_id=repo_id,\n",
    "                                    cache_dir=cache_dir,\n",
    "                                    ignore_patterns=ignore_patterns,\n",
    "                                    )\n",
    "\n",
    "    print(f\"Model downloaded to: {local_path}\")\n",
    "    if has_safetensors:\n",
    "        print(\"Note: PyTorch .bin files were ignored due to the presence of .safetensors files.\")\n",
    "    return os.path.abspath(local_path), has_safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype = torch.bfloat16, device_map = \"cpu\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"models/reduced\", device_map = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.save_pretrained(\"models/target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code has been modified from its original version on the Axolotl project.\n",
    "# Copyright 2023 Axolotl contributors.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "from peft.tuners.lora import QuantLinear\n",
    "\n",
    "\n",
    "def get_linear_embedding_layers(model_type):\n",
    "    \"\"\"\n",
    "    returns the linear embedding layers needed for loras, dependent on the model arch\n",
    "    \"\"\"\n",
    "    if model_type == \"gpt_neox\":\n",
    "        return [\"embed_in\", \"embed_out\"]\n",
    "    if model_type == \"falcon\":\n",
    "        return [\"word_embeddings\", \"lm_head\"]\n",
    "    return [\"embed_tokens\", \"lm_head\"]\n",
    "\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = (bnb.nn.Linear4bit, bnb.nn.Linear8bitLt, torch.nn.Linear, QuantLinear)\n",
    "\n",
    "    names = []\n",
    "    for name, module in model.named_modules():\n",
    "        if (\n",
    "            isinstance(module, cls)\n",
    "            or \"Linear\" in module.__class__.__name__\n",
    "            and module.__class__.__name__ not in (\"LlamaLinearScalingRotaryEmbedding\",)\n",
    "        ):\n",
    "            names.append(name)\n",
    "\n",
    "\n",
    "    return names\n",
    "\n",
    "def get_linear_module_names(model_id):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, state_dict={}, device_map=\"meta\") #avoid loading weights as we won't need them\n",
    "    return find_all_linear_names(model)\n",
    "\n",
    "linear_module_names = get_linear_module_names(\"models/target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _low_rank_decomposition(weight, reduced_rank=16):\n",
    "    \"\"\"\n",
    "    Decompose a 2D matrix into low-rank matrices A and B using SVD.a\n",
    "\n",
    "    :param weight: The matrix to decompose, of shape (H, W)\n",
    "    :param reduced_rank: The final rank of the decomposition\n",
    "    :return: A tuple of tensors (A, B)\n",
    "    \"\"\"\n",
    "    if weight.dim() != 2:\n",
    "        raise ValueError(f\"Only support 2D matrix, but your input has {weight.dim()} dimensions.\")\n",
    "\n",
    "    # SVD Decomposition\n",
    "    U, S, Vh = torch.linalg.svd(weight, full_matrices=False)\n",
    "\n",
    "    # Truncated matrices\n",
    "    A = Vh[:reduced_rank, :]\n",
    "    B = U[:, :reduced_rank] @ torch.diag(S[:reduced_rank])\n",
    "\n",
    "    return A, B\n",
    "\n",
    "def decompose_delta_weight(new_weight, base_weight, alpha, reduced_rank, device=None):\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    new_weight = new_weight.to(device)\n",
    "    base_weight = base_weight.to(device)\n",
    "\n",
    "    \"\"\"\n",
    "    Decompose the delta weight into low-rank matrices A and B, considering the alpha scaling factor.\n",
    "\n",
    "    :param new_weight: The updated weight matrix after applying LoRA.\n",
    "    :param base_weight: The original weight matrix before LoRA.\n",
    "    :param alpha: The alpha scaling factor used in LoRA.\n",
    "    :param reduced_rank: The rank for the low-rank decomposition.\n",
    "    :return: A tuple of tensors (A, B)\n",
    "    \"\"\"\n",
    "    delta_weight = new_weight - base_weight\n",
    "\n",
    "    # Check if alpha is applied uniformly\n",
    "    # Adjust the implementation if alpha is applied differently\n",
    "    adjusted_delta_weight = delta_weight / alpha\n",
    "\n",
    "    A, B = _low_rank_decomposition(adjusted_delta_weight, reduced_rank=reduced_rank)\n",
    "\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "loras = OrderedDict()\n",
    "\n",
    "# lower rank captures less of the original model, a rank of 32 is probably reasonable for small delta (task specific finetunes and such)\n",
    "alpha = 1\n",
    "rank = 64\n",
    "\n",
    "for module in tqdm(linear_module_names):\n",
    "  target_tensor = target_model.state_dict()[module+\".weight\"]\n",
    "  base_tensor = base_model.state_dict()[module+\".weight\"]\n",
    "\n",
    "  lora_A, lora_B = decompose_delta_weight(target_tensor, base_tensor, alpha, rank)\n",
    "  loras[f\"base_model.model.{module}.lora_A.weight\"] = lora_A.to('cpu')\n",
    "  loras[f\"base_model.model.{module}.lora_B.weight\"] = lora_B.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_peft_name(module_name):\n",
    "    return module_name.split('.')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "LORA_OUT_DIR = \"./lora\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "        lora_alpha=64, # Setting the alpha to the to decomposition rank value (instead of alpha value used) seems to give better performance. Further testing would be needed to understand what is the optimal alpha value to use\n",
    "        lora_dropout=0,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= list(set([get_module_peft_name(e) for e in linear_module_names])),\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/reduced\")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Save to disk\n",
    "peft_model.save_pretrained(LORA_OUT_DIR)\n",
    "\n",
    "del peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for key in loras.keys():\n",
    "    loras[key] = loras[key].to('cpu').contiguous()\n",
    "\n",
    "torch.save(loras, os.path.join(LORA_OUT_DIR, 'adapter_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "LORA_OUT_DIR = \"./lora\"\n",
    "target_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(os.path.abspath(LORA_OUT_DIR))\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/reduced\", torch_dtype = torch.bfloat16, device_map = \"cuda\")\n",
    "model = PeftModel.from_pretrained(model, os.path.abspath(LORA_OUT_DIR), device_map = \"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test input\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hey what's up?\"},\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Tokenize and format the chat for the model\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "model = model.bfloat16().cuda()\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(tokenized_chat, max_new_tokens=256, do_sample = True, top_p = 0.95, temperature = 0.6)  # Adjust max_new_tokens if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
